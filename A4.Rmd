---
title: "A4: "
author: "Name and Student Number: Aditya Chauhan 169027493"
---

```{r active="", eval=FALSE}
# BEGIN ASSIGNMENT 
```


```{r}
# DO NOT PUT install.packages() IN A SUBMISSION TO GRADESCOPE!!!
library(tidyverse)
library(ggrepel)
theme_set(theme_bw())
library(arrow)
library(tidymodels)
tidymodels_prefer()
```

In the previous assignments, we worked with the cyclones data. We'll continue that now. Thanks to the magic of parquet files, we can load in the cleaned data without having to re-define the columns or column types.

However, for this analysis we want each row to be a single storm. To do this, we'll group by Name, NameYear, and Basin to uniquely define each storm. From there, we'll have to summarise the measurements so that we have a single row for each storm. 

In the code below, create the following columns (some will need to be in `summarise()`, others will need to be in `mutate()`):

- `start_lat`, `start_lon`, `end_lat`, and `end_lon` are done for you. They are the first and last observation of the lat/lon, respectively. This was achieved by sorting by date (a column we made in A2), then taking the first and last row.
- `start_date` and `end_date`
- `lowest_lat`, `highest_lat`, `lowest_lon`, and `highest_lon` are the min/max lon/lat that the storm was observed.
- `max_wind` is the maximum wind across all observations of the storm.
- `min_pressure` is the minumum pressure across all observations of the storm
- `max_cat` is the highest category rating that the storm was ever given. I did this one for you since `parsnip` does some strange things with ordered factors, and my version of the data has this as being ordered.
- `storm_length` is the total time the storm was observed
- `lat_change` and `lon_change` is the displacement from the first observation of the storm to the last.

There will be warnings about "no non-missing arguments", which are simply from cyclones for which all of the values are `NA` for the relevant column.

```{r cyclones_by_name}
cyclones <- read_parquet("cyclones_data.parquet")

colnames(cyclones)

cyclones_by_name <- cyclones |>
    group_by(Name, NameYear, Basin) |>
    arrange(date) |>
    summarise(
        start_lat = lat[1],
        start_lon = lon[1],
        end_lat = lat[n()],
        end_lon = lon[n()],
        start_date = min(date, na.rm = TRUE),
        end_date = max(date, na.rm = TRUE),
        lowest_lat = min(lat, na.rm = TRUE),
        highest_lat = max(lat, na.rm = TRUE),
        lowest_lon = min(lon, na.rm = TRUE),
        highest_lon = max(lon, na.rm = TRUE),
        max_wind = max(max_wind, na.rm = TRUE),
        min_pressure = min(min_pressure, na.rm = TRUE),
        max_cat = factor(max(category, na.rm = TRUE), ordered = FALSE),
        .groups = "drop"
    ) |>
    mutate(
        storm_length = as.numeric(difftime(end_date, start_date, units = "days")),
        lat_change = end_lat - start_lat,
        lon_change = end_lon - start_lon
    ) |>
    filter(!is.na(min_pressure) & !is.infinite(min_pressure))

cyclones_by_name
```

```{r}
. = ottr::check("tests/cyclones_by_name.R")
```
































Before modelling, we'll set up a training, validation, and test set. Put 60% of the data in the training set, 20% in the validation split, and the remaining 20% will go to the test set. Stratify the sets by `Basin` to ensure that the random assignment preserves the proportion of storms in each basin.

```{r cyclones_split}
cyclones_split <- initial_validation_split(
    cyclones_by_name,
    prop = c(train = 0.6, validation = 0.2),
    strata = Basin
)

cyclones_split
```

```{r}
. = ottr::check("tests/cyclones_split.R")
```



































Using `min_pressure` as the target/response variable, choose three sets of features/predictors from `cyclones_by_name`.

- **Manually graded** (9 marks): create a plot for each model that demonstrates that your choice of features is reasonable. In one sentence for each recipe, explain why the set of features is reasonable. I will check the code to see that a meaningful plot is made.
    - It is totally okay for the recipes to share some/most of the same features as long as you explain why the difference is important/meaningful.
    
```{r}
training_data <- training(cyclones_split)

ggplot(training_data, aes(x = max_wind, y = min_pressure)) +
  geom_point() +
  geom_smooth(method = "lm", color = "blue") +
  labs(title = "Max Wind vs Min Pressure", x = "Max Wind (knots)", y = "Min Pressure (hPa)")

ggplot(training_data, aes(x = max_cat, y = min_pressure)) +
  geom_boxplot() +
  labs(title = "Max Category vs Min Pressure", x = "Max Category", y = "Min Pressure (hPa)")

ggplot(training_data, aes(x = storm_length, y = min_pressure)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red") +
  labs(title = "Storm Length vs Min Pressure", x = "Storm Length (days)", y = "Min Pressure (hPa)")


```


Explainations: 

Explanation for Recipe 1

	1.	Max Wind vs. Min Pressure: The strong negative correlation (as seen from the downward trend in the scatter plot) indicates that higher wind speeds are associated with lower pressure, which aligns with our understanding of storm intensity.
	2.	Max Category vs. Min Pressure: The box plot shows that higher storm categories correspond to lower median pressures. This is meaningful because higher categories represent more severe storms, typically with lower central pressure.
	3.	Storm Length vs. Min Pressure: The scatter plot shows a slight negative trend, suggesting that longer storms tend to have lower pressures. This could indicate that prolonged storm activity is correlated with sustained intensity.


Explanation for Recipe 2

	1.	Latitude Change vs. Min Pressure: The scatter plot would likely show that larger latitude changes correlate with lower pressures. This makes sense as storms that traverse greater latitudinal distances often indicate more intense systems capable of sustaining their structure.
	2.	Longitude Change vs. Min Pressure: Similarly, greater longitude changes could imply that the storm covered a wider area, potentially correlating with stronger and more sustained pressure drops.
	3.	Lowest Latitude vs. Min Pressure: The plot would show that storms reaching lower latitudes tend to have lower pressures, reflecting that tropical regions often foster stronger cyclonic activity.
	
	
Explanation for Recipe 3

	1.	Starting Latitude vs. Min Pressure: The scatter plot would demonstrate that storms originating at certain latitudes (likely lower ones) are associated with lower pressures, as these regions provide the warm waters necessary for storm intensification.
	2.	Starting Longitude vs. Min Pressure: This plot might show that storms starting in certain longitudes (e.g., regions prone to cyclonic activity) are associated with lower pressures, reflecting regional weather patterns.
	3.	Highest Longitude vs. Min Pressure: The plot would reveal how far east or west a storm travels, potentially correlating with the systemâ€™s strength and, hence, the observed minimum pressure.


- **Autograded**: `cyclones_recipes` should contain three distinct recipes using features from `cyclones_by_name`, each with at least 3 features. 
    - Make sure you use dummy variables as appropriate!

For all of the recipes, use the *training data* as the data set inside of `recipe`.

Use the `preproc <- list()` example from Chapter 11 of the [tmwr text](https://www.tmwr.org/compare) as a guide, but name the list as `cyclones_recipes` rather than `preproc` and give your recipes meaningful names. 

```{r cyclones_recipes}
cyclones_recipes <- list(
    recipe_1 = recipe(min_pressure ~ max_wind + max_cat + storm_length, data = training(cyclones_split)),
    recipe_2 = recipe(min_pressure ~ lat_change + lon_change + lowest_lat + highest_lat, data = training(cyclones_split)),
    recipe_3 = recipe(min_pressure ~ start_lat + start_lon + max_cat + highest_lon, data = training(cyclones_split))
)

cyclones_recipes
```

```{r}
. = ottr::check("tests/cyclones_recipes.R")
```
































Now we'll set up the workflow! Use `workflow_set()` to make a workflow set object that includes a linear model (with the `"lm"` engine) that includes the list of recipes from above.

```{r cyclones_workflow_set}
cyclones_workflow_set <- workflow_set(
    preproc = cyclones_recipes,
    models = list(lm = linear_reg() |> set_engine("lm"))
)

cyclones_workflow_set
```

```{r}
. = ottr::check("tests/cyclones_workflow_set.R")
```

















Now let's fit to the training data! Use the `workflow_map()` function to fit all of the models. Use the `"tune_grid"` function (`fn`) to fit the models, making sure to set the seed. You'll need to set the "resamples" argument, which in this case means calculating a `validation_set()` from the split object made earlier.

```{r cyclones_fit}
validation_split <- validation_set(cyclones_split)

set.seed(42)
cyclones_fit <- cyclones_workflow_set |>
    workflow_map(
        fn = "fit_resamples",
        resamples = validation_split,
        metrics = metric_set(rmse, rsq)
    )

cyclones_fit

# Also plot the results for you to see.
cyclones_fit |>
    collect_metrics() |>
    ggplot() +
    aes(x = wflow_id, y = mean) +
    geom_col(fill = "lightgrey", colour = "black") +
    facet_wrap(~ .metric, scales = "free")
```

```{r}
. = ottr::check("tests/cyclones_fit.R")
```

































We'll also add some random forests into the mix. The details are not covered in this class, but in order to fit a random forest we have to **tune** a **hyperparameter**. A hyperparameter is a part of a model that *can't* be chosen based on the data, but need a **resampling method**, such as [$k$-fold cross-validation](https://www.tmwr.org/resampling#cv), to choose a parameter such that the out-of-sample prediction error is minimized.

In the code below, fill in the `preproc` and `models` arguments of the `workflow_set()` function, where the preprocessing is the same as before and the models should be a list with names `lm` and `rf`. The rest of the work is done for you, so sit back and enjoy!

Warning: It takes a little while to run!

```{r lm_versus_rf}
train_test <- initial_split(cyclones_by_name, prop = 0.8)

rf <- rand_forest(trees = 1000, mtry = tune(), min_n = 3) |>
    set_engine("randomForest") |>
    set_mode("regression")
tune_grid <- tibble(mtry = 1:3)

lm_versus_rf <- workflow_set(
    preproc = cyclones_recipes,
    models = list(lm = linear_reg() |> set_engine("lm"), rf = rf)
) |>
    workflow_map(
        fn = "tune_grid",
        grid = tune_grid,
        seed = 100,
        control = control_grid(save_pred = TRUE, save_workflow = TRUE),
        resamples = vfold_cv(training(cyclones_split), v = 5)
    )

lm_versus_rf |>
    autoplot(select_best = TRUE, metric = "rmse") +
    geom_label_repel(aes(label = wflow_id))

```

```{r}
. = ottr::check("tests/lm_versus_rf.R")
```













In the code chunk below, we'll select the best model and then finalize it. Use the following steps:

1. Choose the linear model and the random forest model that you believe are the "best".
    - Use the plot above (the one made by "autoplot") and possibly the `collect_metrics()` function.
    - Use your judgement! Consider the RMSE, Rsq, as well as the complexity of the model and the number of features to make your choice.
2. Use `extract_workflow_set_result()` to get the best linear and random forest model from `lm_versus_rf`. 
3. Use `select_best()` to find the hyperparameter combination for each model that fits the validation set the best. Save this to an object, e.g. `best_lm_workflow`.
    - This is an important point: the training/validation/test framework allows us to find the best *method* for model fitting, not just the best parameter combination. This way, we know how to use the training data to make predictions on the test data, even though we've never seen it before!

The output should be two R objects: one for the best linear model and one for the best random forest. These should be the best *workflow*, do not fit the model yet.

```{r best_model, error=TRUE}
best_lm <- lm_versus_rf |>
    extract_workflow("recipe_1_lm")

best_rf <- lm_versus_rf |>
    extract_workflow("recipe_2_rf")
```

```{r}
. = ottr::check("tests/best_model.R")
```
























Now let's see how they do on a test set! use the `last_fit()` function (you will only need the `last_fit()` function for this step), which will use the whole *training set* (including *validation set*) to fit the model, then will report the RMSE and $R^2$ for the *test set*.

In contrast to the previous step in which we found the best method for estimating our parameters, we're now estimating the final parameters!

```{r test_fit}
best_rf_hyperparameters <- lm_versus_rf |>
    extract_workflow_set_result("recipe_2_rf") |>
    select_best(metric = "rmse")

best_rf <- best_rf |> finalize_workflow(best_rf_hyperparameters)

test_lm <- best_lm |> last_fit(cyclones_split)
test_rf <- best_rf |> last_fit(cyclones_split)

cat("\nBest RMSE and R^2:\n")
bind_rows(
    lm = collect_metrics(test_lm),
    rf = collect_metrics(test_rf),
    .id = "model"
)

cat("\nPredictors used:\n")
extract_recipe(test_lm) |> formula()
extract_recipe(test_rf) |> formula()

# Plot the predictions versus observed
bind_rows(
    lm = collect_predictions(test_lm),
    rf = collect_predictions(test_rf),
    .id = "model"
) |>
    ggplot() +
    aes(x = min_pressure, y = .pred, colour = model) +
    geom_point(shape = 1)

# Plot the metrics
bind_rows(
    lm = test_lm,
    rf = test_rf,
    .id = "model"
) |>
    # This is almost certainly a better way to do this, but I do not know it.
    unnest_wider(.metrics) |>
    unnest_longer(c(.metric, .estimator, .estimate, .config)) |>
    ggplot() +
    aes(x = model, y = .estimate) +
    geom_col(fill = "lightgrey", color = 1) +
    facet_wrap(~ .metric, scales = "free_y")
```

```{r}
. = ottr::check("tests/test_fit.R")
```

**Manually Graded** (4 marks): Using the information above and throughout this assignment, which model do you think is best?

The linear model outperforms the random forest in RMSE & in R^2, it provides more consistent and reliable predictions, it better captures the underlying relationships, and as an added plus, it is also more simple, so it is the better choice. The dataset has simple relationships and random forest struggles with the noise too. 


```{r active="", eval=FALSE}
# END ASSIGNMENT 
```
