---
title: "A4: "
author: "Name and Student Number: "
---

```{r active="", eval=FALSE}
# BEGIN ASSIGNMENT 
```


```{r}
# DO NOT PUT install.packages() IN A SUBMISSION TO GRADESCOPE!!!
library(tidyverse)
library(ggrepel)
theme_set(theme_bw())
library(arrow)
library(tidymodels)
tidymodels_prefer()
```

In the previous assignments, we worked with the cyclones data. We'll continue that now. Thanks to the magic of parquet files, we can load in the cleaned data without having to re-define the columns or column types.

However, for this analysis we want each row to be a single storm. To do this, we'll group by Name, NameYear, and Basin to uniquely define each storm. From there, we'll have to summarise the measurements so that we have a single row for each storm. 

In the code below, create the following columns (some will need to be in `summarise()`, others will need to be in `mutate()`):

- `start_lat`, `start_lon`, `end_lat`, and `end_lon` are done for you. They are the first and last observation of the lat/lon, respectively. This was achieved by sorting by date (a column we made in A2), then taking the first and last row.
- `start_date` and `end_date`
- `lowest_lat`, `highest_lat`, `lowest_lon`, and `highest_lon` are the min/max lon/lat that the storm was observed.
- `max_wind` is the maximum wind across all observations of the storm.
- `min_pressure` is the minumum pressure across all observations of the storm
- `max_cat` is the highest category rating that the storm was ever given. I did this one for you since `parsnip` does some strange things with ordered factors, and my version of the data has this as being ordered.
- `storm_length` is the total time the storm was observed
- `lat_change` and `lon_change` is the displacement from the first observation of the storm to the last.

There will be warnings about "no non-missing arguments", which are simply from cyclones for which all of the values are `NA` for the relevant column.

```{r cyclones_by_name}
cyclones <- read_parquet("cyclones_data.parquet")

cyclones_by_name <- cyclones |>
    group_by(Name, NameYear, Basin) |>
    arrange(date) |>
    summarise(
        start_lat = lat[1],
        start_lon = lon[1],
        end_lat = lat[n()],
        end_lon = lon[n()],
        # YOUR CODE HERE
        max_cat = factor(max(category, na.rm = TRUE), ordered = FALSE),
        .groups = "drop"
    ) |>
    mutate(
        # YOUR CODE HERE
    ) |>
    filter(!is.na(min_pressure) & !is.infinite(min_pressure))

cyclones_by_name
```

```{r}
. = ottr::check("tests/cyclones_by_name.R")
```
































Before modelling, we'll set up a training, validation, and test set. Put 60% of the data in the training set, 20% in the validation split, and the remaining 20% will go to the test set. Stratify the sets by `Basin` to ensure that the random assignment preserves the proportion of storms in each basin.

```{r cyclones_split}
# YOUR CODE HERE

cyclones_split
```

```{r}
. = ottr::check("tests/cyclones_split.R")
```



































Using `min_pressure` as the target/response variable, choose three sets of features/predictors from `cyclones_by_name`.

- **Manually graded** (9 marks): create a plot for each model that demonstrates that your choice of features is reasonable. In one sentence for each recipe, explain why the set of features is reasonable. I will check the code to see that a meaningful plot is made.
    - It is totally okay for the recipes to share some/most of the same features as long as you explain why the difference is important/meaningful.
- **Autograded**: `cyclones_recipes` should contain three distinct recipes using features from `cyclones_by_name`, each with at least 3 features. 
    - Make sure you use dummy variables as appropriate!

For all of the recipes, use the *training data* as the data set inside of `recipe`.

Use the `preproc <- list()` example from Chapter 11 of the [tmwr text](https://www.tmwr.org/compare) as a guide, but name the list as `cyclones_recipes` rather than `preproc` and give your recipes meaningful names. 

```{r cyclones_recipes}
# YOUR CODE HERE

cyclones_recipes
```

```{r}
. = ottr::check("tests/cyclones_recipes.R")
```
































Now we'll set up the workflow! Use `workflow_set()` to make a workflow set object that includes a linear model (with the `"lm"` engine) that includes the list of recipes from above.

```{r cyclones_workflow_set}
# YOUR CODE HERE

cyclones_workflow_set
```

```{r}
. = ottr::check("tests/cyclones_workflow_set.R")
```

















Now let's fit to the training data! Use the `workflow_map()` function to fit all of the models. Use the `"tune_grid"` function (`fn`) to fit the models, making sure to set the seed. You'll need to set the "resamples" argument, which in this case means calculating a `validation_set()` from the split object made earlier.

```{r cyclones_fit}
# YOUR CODE HERE

cyclones_fit

# Also plot the results for you to see.
cyclones_fit |>
    collect_metrics() |>
    ggplot() +
    aes(x = wflow_id, y = mean) +
    geom_col(fill = "lightgrey", colour = "black") +
    facet_wrap(~ .metric, scales = "free")
```

```{r}
. = ottr::check("tests/cyclones_fit.R")
```

































We'll also add some random forests into the mix. The details are not covered in this class, but in order to fit a random forest we have to **tune** a **hyperparameter**. A hyperparameter is a part of a model that *can't* be chosen based on the data, but need a **resampling method**, such as [$k$-fold cross-validation](https://www.tmwr.org/resampling#cv), to choose a parameter such that the out-of-sample prediction error is minimized.

In the code below, fill in the `preproc` and `models` arguments of the `workflow_set()` function, where the preprocessing is the same as before and the models should be a list with names `lm` and `rf`. The rest of the work is done for you, so sit back and enjoy!

Warning: It takes a little while to run!

```{r lm_versus_rf}
train_test <- initial_split(cyclones_by_name, prop = 0.8)

rf <- rand_forest(trees = 1000, mtry = tune(), min_n = 3) |>
    set_engine("randomForest") |>
    set_mode("regression")
tune_grid <- tibble(mtry = 1:3)

lm_versus_rf <- workflow_set(
    # YOUR CODE HERE
) |>
    workflow_map(
        fn = "tune_grid",
        grid = tune_grid,
        seed = 100,
        control = control_grid(save_pred = TRUE, save_workflow = TRUE),
        resamples = vfold_cv(training(cyclones_split), v = 5)
    )

lm_versus_rf |>
    autoplot(select_best = TRUE, metric = "rmse") +
    geom_label_repel(aes(label = wflow_id))

```

```{r}
. = ottr::check("tests/lm_versus_rf.R")
```













In the code chunk below, we'll select the best model and then finalize it. Use the following steps:

1. Choose the linear model and the random forest model that you believe are the "best".
    - Use the plot above (the one made by "autoplot") and possibly the `collect_metrics()` function.
    - Use your judgement! Consider the RMSE, Rsq, as well as the complexity of the model and the number of features to make your choice.
2. Use `extract_workflow_set_result()` to get the best linear and random forest model from `lm_versus_rf`. 
3. Use `select_best()` to find the hyperparameter combination for each model that fits the validation set the best. Save this to an object, e.g. `best_lm_workflow`.
    - This is an important point: the training/validation/test framework allows us to find the best *method* for model fitting, not just the best parameter combination. This way, we know how to use the training data to make predictions on the test data, even though we've never seen it before!

The output should be two R objects: one for the best linear model and one for the best random forest. These should be the best *workflow*, do not fit the model yet.

```{r best_model, error=TRUE}

best_lm_workflow <- lm_versus_rf |>
    extract_workflow_set_result("PUT THE NAME OF YOUR FINAL MODEL HERE") |>
    select_best(metric = "rmse")

# YOUR CODE HERE

```

```{r}
. = ottr::check("tests/best_model.R")
```
























Now let's see how they do on a test set! use the `last_fit()` function (you will only need the `last_fit()` function for this step), which will use the whole *training set* (including *validation set*) to fit the model, then will report the RMSE and $R^2$ for the *test set*.

In contrast to the previous step in which we found the best method for estimating our parameters, we're now estimating the final parameters!

```{r test_fit}
# YOUR CODE HERE

cat("\nBest RMSE and R^2:\n")
bind_rows(
    lm = collect_metrics(test_lm),
    rf = collect_metrics(test_rf),
    .id = "model"
)

cat("\nPredictors used:\n")
extract_recipe(test_lm) |> formula()
extract_recipe(test_rf) |> formula()

# Plot the predictions versus observed
bind_rows(
    lm = collect_predictions(test_lm),
    rf = collect_predictions(test_rf),
    .id = "model"
) |>
    ggplot() +
    aes(x = min_pressure, y = .pred, colour = model) +
    geom_point(shape = 1)

# Plot the metrics
bind_rows(
    lm = test_lm,
    rf = test_rf,
    .id = "model"
) |>
    # This is almost certainly a better way to do this, but I do not know it.
    unnest_wider(.metrics) |>
    unnest_longer(c(.metric, .estimator, .estimate, .config)) |>
    ggplot() +
    aes(x = model, y = .estimate) +
    geom_col(fill = "lightgrey", color = 1) +
    facet_wrap(~ .metric, scales = "free_y")
```

```{r}
. = ottr::check("tests/test_fit.R")
```

**Manually Graded** (4 marks): Using the information above and throughout this assignment, which model do you think is best?


```{r active="", eval=FALSE}
# END ASSIGNMENT 
```
